{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from mvcomp import *\n",
    "\n",
    "import os\n",
    "from os.path import join, basename\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make lists of subjects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# For one-sample analysis, only one list is needed\n",
    "\n",
    "# For two-sample analysis (where the reference is a specific group, e.g., control): \n",
    "# make a list for the reference group and a list of subjects you want to compute D2 for\n",
    "\n",
    "\n",
    "# Let's use the example case of one-sample analysis here (with the leave-one-subject-out approach)\n",
    "\n",
    "# Directory containing the folders of each subject (e.g., '/project/data/Subject_001/')\n",
    "in_dir = '/project/data/'\n",
    "\n",
    "# P_folders is a list with the full path of each subject\n",
    "P_folders = glob.glob(join(in_dir,'*'))\n",
    "\n",
    "ID_list = []\n",
    "\n",
    "for i in P_folders: \n",
    "    # PID is the basename of every element in P_folders. ID_list will be a list containing all PIDs\n",
    "    PID = basename(i)\n",
    "    ID_list.append(PID)\n",
    "    \n",
    "print(len(ID_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the reference mean and covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-8-381f32613b9a>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-381f32613b9a>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    compute_average(ids= ID_list, in_dir= feature_dir, out_dir, features,\u001b[0m\n\u001b[0m                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "# Compute average of all subjects (for each feature) using compute_average function\n",
    "# Since we're using the leave-one-subject-out approach, we will only use the average maps for getting the correlation matrix (to visualize relationships between metrics)\n",
    "# Skip this cell if you are not interested in visualizing the correlation matrix\n",
    "\n",
    "feature_dir = '/project/data/'\n",
    "out_dir = '/project/data/Group_average/'\n",
    "features = ['CSD_AFDtotal', 'CSD_meanFC', 'CSD_sumFDC', 'DTI_AD', 'DTI_FA', 'DTI_MD', 'DTI_RD', 'MPM_MTsat', 'MPM_PD', 'MPM_R1', 'MPM_R2', 'NODDI_ICVF', 'NODDI_ISOVF', 'NODDI_OD', 'T1_div_by_T2']\n",
    "feature_suffix = '_warped_group_space.nii.gz' # features and feature_suffix should create the file names for the features wanted\n",
    "\n",
    "# We provided the compute_average function to create custom averages when desired\n",
    "compute_average(ids= ID_list, in_dir= feature_dir, out_dir, features,\n",
    "                        feature_suffix, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of feature names and a list of paths for the reference images (this list will be used for computation of correlation matrix)\n",
    "\n",
    "reference_dir = '/project/data/Group_average/'\n",
    "reference_suffix = '_xxaverage.nii.gz' # edit this with the suffix of the images outputted in previous step (the suffix depends on the number of subjects contained in the average)\n",
    "excluded_features = []  # add names of features you want to exclude (e.g., 'T1_div_by_T2') \n",
    "\n",
    "# get reference path and file names\n",
    "reference_fullpath_list, reference_fname_list = feature_list(reference_dir, \n",
    "                                                    reference_suffix, excluded_features)\n",
    "\n",
    "print(reference_fname_list)\n",
    "print(reference_fullpath_list)\n",
    "\n",
    "# Create feature matrix of the reference within a mask (and using a threshold) \n",
    "\n",
    "# Define mask\n",
    "mask_f = '/project/data/mask/white_matter_mask.nii.gz'\n",
    "\n",
    "# generate matrices of the reference features and the masks, with a desired threshold\n",
    "m_f_mat, mask_img, mat_mask = feature_gen(reference_fullpath_list, mask_image_fname = mask_f, \n",
    "                                            mask_threshold = 0.9)\n",
    "\n",
    "print(\"reference feature matrix shape is: \", m_f_mat.shape)\n",
    "# m_f_mat is the feature matrix (from the reference)\n",
    "# mask_img is a nibabel object of the mask\n",
    "# mat_mask is a boolian numpy array of the mask with the value of zero in the inf\\nan voxels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can plot the covariance and correlation of your features should you want to inspect them \n",
    "# Calculate covariance (s) and inverse of covariance (pinv_s) of the reference's feature matrix\n",
    "s, pinv_s = norm_covar_inv(m_f_mat, mat_mask)\n",
    "\n",
    "# Show correlation coefficient map \n",
    "correlation_fig(s, reference_fname_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute D2 (with leave-one-out approach; exclude_comp_from_mean_cov= True)\n",
    "# At the moment, model_dir and suffix_name_model are needed just to get features names (this has to be fixed)\n",
    "\n",
    "result_dict = model_comp(feature_in_dir= feature_dir, model_dir= None, \n",
    "                                 suffix_name_comp= feature_suffix, \n",
    "                                 exclude_comp_from_mean_cov= True, suffix_name_model= reference_suffix, \n",
    "                                 mask_f= mask_f, mask_img= None, verbosity=1, \n",
    "                                 mask_threshold=0.99, subject_ids= ID_list, \n",
    "                                 exclude_subject_ids= None, feat_sub= excluded_features, \n",
    "                                 return_raw=False)\n",
    "\n",
    "# results are stored in a dictionary, can access array of D2 data (of all subjects) like this:\n",
    "all_D2 = result_dict['all_dist']\n",
    "print(all_D2.shape) # Shape will be (number of voxels) x (number of subjects)\n",
    "\n",
    "# Save matrix containing all D2 data as a numpy array\n",
    "\n",
    "np.save('/project/Analyses/Reference_all_subjects/D2_all_subjects.npy', all_D2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the D2 maps (images)\n",
    "# Will also plot the mean D2 map and D2 histogram of all subjects \n",
    "# Right now mask_img is needed (which is created with feature_gen function)\n",
    "\n",
    "subject_ids = ID_list\n",
    "\n",
    "dist_plot(all_D2, all_masks, subject_ids, excluded_features, save_results=True, \n",
    "              out_dir = '/project/Analyses/Reference_all_subjects/', \n",
    "              mask_f=None, mask_img=mask_img, coordinate=(-10, -50, 10), vmin = 0, vmax = 70, hist_tr = 120, nobin = 200)\n",
    "\n",
    "# Change name of directory created if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting distances per feature (raw) using the return_raw=True option of model_comp\n",
    "# Note that this step can be combined with the previous one, but it will increase computational time.\n",
    "\n",
    "result_dict = model_comp(feature_in_dir= feature_dir, model_dir= None, \n",
    "                                 suffix_name_comp= feature_suffix, \n",
    "                                 exclude_comp_from_mean_cov= True, suffix_name_model= reference_suffix, \n",
    "                                 mask_f= mask_f, mask_img= None, verbosity=1, \n",
    "                                 mask_threshold=0.99, subject_ids= ID_list, \n",
    "                                 exclude_subject_ids= None, feat_sub= excluded_features, \n",
    "                                 return_raw=True)\n",
    "\n",
    "\n",
    "raw_D2 = result_dict['raw_dist'] \n",
    "print(raw_D2.shape) # Shape will be (number of voxels) x (number of features) x (number of subjects)\n",
    "\n",
    "# The raw distances can then be summarized to obtain the relative contribution of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_fig_big(s, f_list):\n",
    "    '''\n",
    "    Plot the correlation table from the covariance matrix.\n",
    "    \n",
    "    Args: \n",
    "        s (numpy.ndarray): covariance matrix of size feature x feature\n",
    "        f_list (List of strings): A list of the names of the features that should be in the same order of the covarinca matrix.\n",
    "        \n",
    "    Return:\n",
    "        Non.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    r = np.zeros(s.shape)\n",
    "    for i in range(s.shape[0]):\n",
    "        for j in range(s.shape[1]):\n",
    "            r[i, j] = s[i, j] / (math.sqrt(s[i, i] * s[j, j]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(11.2, 8.4))\n",
    "    im = ax.imshow(r, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    title = ax.set_title('Metric-Metric Correlation Matrix')\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set_xticks(np.arange(s.shape[1]))\n",
    "    ax.set_yticks(np.arange(s.shape[0]))\n",
    "    xtl = ax.set_xticklabels(f_list)\n",
    "    ytl = ax.set_yticklabels(f_list)  # Let the horizontal axes labeling appear on top.\n",
    "    _ = ax.tick_params(top=True, bottom=False,\n",
    "                       labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    _ = plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(f_list)):\n",
    "        for j in range(len(f_list)):\n",
    "            if i == j:\n",
    "                pass\n",
    "            else:\n",
    "                val = r[i, j]\n",
    "                if val > 0.6:\n",
    "                    cc = 'w'\n",
    "                else:\n",
    "                    cc = 'k'\n",
    "                text = ax.text(j, i, \"{:.2f}\".format(val),\n",
    "                               ha=\"center\", va=\"center\", color=cc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
